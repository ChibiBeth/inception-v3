{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Program Files\\Python37\\python.exe' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Program Files/Python37/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import os.path\n",
    "import sys\n",
    "import operator\n",
    "import threading\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "class DataSet():\n",
    "\n",
    "    def __init__(self, seq_length=40, class_limit=None, image_shape=(224, 224, 3)):\n",
    "        self.seq_length = seq_length\n",
    "        self.class_limit = class_limit\n",
    "        self.sequence_path = os.path.join('data', 'sequences')\n",
    "        self.max_frames = 300  # max number of frames a video can have for us to use it\n",
    "        self.data = self.get_data()\n",
    "        self.classes = self.get_classes()\n",
    "        self.data = self.clean_data()\n",
    "        self.image_shape = image_shape\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data():\n",
    "        with open(os.path.join('data', 'data_file.csv'), 'r') as fin:\n",
    "            reader = csv.reader(fin)\n",
    "            data = list(reader)\n",
    "        return data\n",
    "\n",
    "    def clean_data(self):\n",
    "        data_clean = []\n",
    "        for item in self.data:\n",
    "            if int(item[3]) >= self.seq_length and int(item[3]) <= self.max_frames \\\n",
    "                    and item[1] in self.classes:\n",
    "                data_clean.append(item)\n",
    "\n",
    "        return data_clean\n",
    "\n",
    "    def get_classes(self):\n",
    "        classes = []\n",
    "        for item in self.data:\n",
    "            if item[1] not in classes:\n",
    "                classes.append(item[1])\n",
    "        classes = sorted(classes)\n",
    "        if self.class_limit is not None:\n",
    "            return classes[:self.class_limit]\n",
    "        else:\n",
    "            return classes\n",
    "\n",
    "    def get_class_one_hot(self, class_str):\n",
    "        # Encode it first.\n",
    "        label_encoded = self.classes.index(class_str)\n",
    "        # Now one-hot it.\n",
    "        label_hot = to_categorical(label_encoded, len(self.classes))\n",
    "        assert len(label_hot) == len(self.classes)\n",
    "        return label_hot\n",
    "\n",
    "    def split_train_test(self):\n",
    "        train = []\n",
    "        test = []\n",
    "        for item in self.data:\n",
    "            if item[0] == 'train':\n",
    "                train.append(item)\n",
    "            else:\n",
    "                test.append(item)\n",
    "        return train, test\n",
    "\n",
    "    def get_all_sequences_in_memory(self, train_test, data_type):\n",
    "        train, test = self.split_train_test()\n",
    "        data = train if train_test == 'train' else test\n",
    "\n",
    "        print(\"Loading %d samples into memory for %sing.\" % (len(data), train_test))\n",
    "\n",
    "        X, y = [], []\n",
    "        for row in data:\n",
    "            sequence = self.get_extracted_sequence(data_type, row)\n",
    "            if sequence is None:\n",
    "                print(\"Can't find sequence. Did you generate them?\")\n",
    "                raise\n",
    "            X.append(sequence)\n",
    "            y.append(self.get_class_one_hot(row[1]))\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    def get_extracted_sequence(self, data_type, sample):\n",
    "        filename = sample[2]\n",
    "        path = os.path.join(self.sequence_path, filename + '-' + str(self.seq_length) + \\\n",
    "            '-' + data_type + '.npy')\n",
    "        if os.path.isfile(path):\n",
    "            return np.load(path)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_frames_by_filename(self, filename, data_type):\n",
    "        sample = None\n",
    "        for row in self.data:\n",
    "            if row[2] == filename:\n",
    "                sample = row\n",
    "                break\n",
    "        if sample is None:\n",
    "            raise ValueError(\"Couldn't find sample: %s\" % filename)\n",
    "        sequence = self.get_extracted_sequence(data_type, sample)\n",
    "        if sequence is None:\n",
    "            raise ValueError(\"Can't find sequence. Did you generate them?\")\n",
    "        return sequence\n",
    "\n",
    "    @staticmethod\n",
    "    def get_frames_for_sample(sample):\n",
    "        \"\"\"Given a sample row from the data file, get all the corresponding frame\n",
    "        filenames.\"\"\"\n",
    "        path = os.path.join('data', sample[0], sample[1])\n",
    "        filename = sample[2]\n",
    "        images = sorted(glob.glob(os.path.join(path, filename + '*jpg')))\n",
    "        return images\n",
    "\n",
    "    @staticmethod\n",
    "    def rescale_list(input_list, size):\n",
    "        assert len(input_list)>= size\n",
    "        skip = len(input_list) // size\n",
    "        output = [input_list[i] for i in range(0, len(input_list), skip)]\n",
    "        return output[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "from keras.preprocessing import image as Img\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Get the dataset.\n",
    "#seq_length = 40\n",
    "data = DataSet(seq_length=40, class_limit=2)\n",
    "#print(data)\n",
    "base_model = InceptionV3(\n",
    "    weights='imagenet',\n",
    "    include_top=True\n",
    ")\n",
    "# We'll extract features at the final pool layer.\n",
    "model = Model(\n",
    "    inputs=base_model.input,\n",
    "    outputs=base_model.get_layer('avg_pool').output\n",
    ")\n",
    "\n",
    "# Loop through data.\n",
    "pbar = tqdm(total=len(data.data))\n",
    "for video in data.data:\n",
    "    # Get the path to the sequence for this video.\n",
    "    path = os.path.join('data', 'sequences', video[2] + '-' + str(seq_length) + \\\n",
    "        '-features')  # numpy will auto-append .npy\n",
    "    # Check if we already have it.\n",
    "    if os.path.isfile(path + '.npy'):\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "\n",
    "    # Get the frames for this video.\n",
    "    frames = data.get_frames_for_sample(video)\n",
    "    #print(frames)\n",
    "\n",
    "    # Now downsample to just the ones we need.\n",
    "    frames = data.rescale_list(frames, 40)\n",
    "    #print(frames)\n",
    "    #extracting features and appending to build the sequence.\n",
    "    sequence = []\n",
    "    for image in frames:\n",
    "        img = Img.load_img(image, target_size=(299, 299))\n",
    "        x = Img.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        features = model.predict(x)\n",
    "        sequence.append(features[0])\n",
    "\n",
    "    # Save the sequence.\n",
    "    np.save(path, sequence)\n",
    "\n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten, Dropout, ZeroPadding3D\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import sys\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "import time\n",
    "import os.path\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath=os.path.join('data', 'checkpoints','lstm-features' + '.{epoch:03d}-{val_loss:.3f}.hdf5'),\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "\n",
    "# Helper: TensorBoard\n",
    "tb = TensorBoard(log_dir=os.path.join('data', 'logs', 'lstm'))\n",
    "\n",
    "# Helper: Stop when we stop learning.\n",
    "early_stopper = EarlyStopping(patience=10)\n",
    "\n",
    "# Helper: Save results.\n",
    "timestamp = time.time()\n",
    "csv_logger = CSVLogger(os.path.join('data', 'logs', 'lstm' + '-' + 'training-' + \\\n",
    "    str(timestamp) + '.log'))\n",
    "\n",
    "# Get the data and process it.\n",
    "data = DataSet(\n",
    "    seq_length=40,\n",
    "    class_limit=70\n",
    ")\n",
    "#listt=[]\n",
    "#listt2=[]\n",
    "X, y = data.get_all_sequences_in_memory('train', 'features')\n",
    "X_test, y_test = data.get_all_sequences_in_memory('test', 'features')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(2048, return_sequences=False,input_shape=(40,2048),dropout=0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(data.classes), activation='softmax'))\n",
    "optimizer = Adam(lr=1e-5, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
    "                    metrics=['accuracy','top_k_categorical_accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1,\n",
    "    callbacks=[tb, early_stopper, csv_logger,checkpointer],\n",
    "    epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "from keras.preprocessing import image as Img\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input\n",
    "import glob\n",
    "\n",
    "def rescale_list(input_list, size):\n",
    "    assert len(input_list) >= size\n",
    "    skip = len(input_list) // size\n",
    "    output = [input_list[i] for i in range(0, len(input_list), skip)]\n",
    "    return output[:size]\n",
    "classes = glob.glob(\"data/train/*\")\n",
    "classes = [classes[i].split('/')[2] for i in range(len(classes))]\n",
    "classes = sorted(classes)\n",
    "\n",
    "import cv2 \n",
    "import os \n",
    "image_name = '9.avi'\n",
    "cam = cv2.VideoCapture(image_name) \n",
    "currentframe = 0\n",
    "  \n",
    "frames=[]\n",
    "while(True): \n",
    "    ret,frame = cam.read() \n",
    "    if ret: \n",
    "        # if video is still left continue creating images \n",
    "        name = 'testFinal/frame'+'9' +\"frame_no\"+ str(currentframe) + '.jpg'\n",
    "        cv2.imwrite(name, frame) \n",
    "        frames.append(name)  \n",
    "        currentframe += 1\n",
    "    else: \n",
    "        break\n",
    "cam.release() \n",
    "cv2.destroyAllWindows()\n",
    "rescaled_list = rescale_list(frames,40)\n",
    "\n",
    "base_model = InceptionV3(\n",
    "    weights='imagenet',\n",
    "    include_top=True\n",
    ")\n",
    "# We'll extract features at the final pool layer.\n",
    "inception_model = Model(\n",
    "    inputs=base_model.input,\n",
    "    outputs=base_model.get_layer('avg_pool').output\n",
    ")\n",
    "sequence = []\n",
    "for image in rescaled_list:\n",
    "        img = Img.load_img(image, target_size=(299, 299))\n",
    "        x = Img.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        features = inception_model.predict(x)\n",
    "        sequence.append(features[0])\n",
    "\n",
    "sequence = np.array([sequence])\n",
    "prediction = model.predict(sequence)\n",
    "maxm = prediction[0][0]\n",
    "maxid = 0\n",
    "for i in range(len(prediction[0])):\n",
    "      if(maxm<prediction[0][i]):\n",
    "            maxm = prediction[0][i]\n",
    "            maxid = i\n",
    "#print(frames)\n",
    "print(image_name,' ------- ',classes[maxid])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5109d816b82be14675a6b11f8e0f0d2e80f029176ed3710d54e125caa8520dfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
